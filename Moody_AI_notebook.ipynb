{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erg6ZG68oiJZ",
        "outputId": "443bfd23-d4bc-4df6-efd9-de5e4cc4f55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0                                i didnt feel humiliated      0\n",
            "1      i can go from feeling so hopeless to so damned...      0\n",
            "2       im grabbing a minute to post i feel greedy wrong      3\n",
            "3      i am ever feeling nostalgic about the fireplac...      2\n",
            "4                                   i am feeling grouchy      3\n",
            "...                                                  ...    ...\n",
            "15995  i just had a very brief time in the beanbag an...      0\n",
            "15996  i am now turning and i feel pathetic that i am...      0\n",
            "15997                     i feel strong and good overall      1\n",
            "15998  i feel like this was such a rude comment and i...      3\n",
            "15999  i know a lot but i feel so stupid because i ca...      0\n",
            "\n",
            "[16000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/training.csv'\n",
        "data = pd.read_csv(file_path, sep=',')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrXtS0Ia9viz",
        "outputId": "8c4b5530-b222-4241-8952-5ae7ce0e0d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P_mzFdS9zmV",
        "outputId": "6f8a18c4-7d37-4c66-e0dc-9d1b95dab8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "1    5362\n",
            "0    4666\n",
            "3    2159\n",
            "4    1937\n",
            "2    1304\n",
            "5     572\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "emotion_counts = data['label'].value_counts()\n",
        "print(emotion_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw1dwNvk3ejA",
        "outputId": "7fb7d88e-69c3-471f-e470-59aed792bc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label\n",
            "0     im feeling rather rotten so im not very ambiti...      0\n",
            "1             im updating my blog because i feel shitty      0\n",
            "2     i never make her separate from me because i do...      0\n",
            "3     i left with my bouquet of red and yellow tulip...      1\n",
            "4       i was feeling a little vain when i did this one      0\n",
            "...                                                 ...    ...\n",
            "1995  i just keep feeling like someone is being unki...      3\n",
            "1996  im feeling a little cranky negative after this...      3\n",
            "1997  i feel that i am useful to my people and that ...      1\n",
            "1998  im feeling more comfortable with derby i feel ...      1\n",
            "1999  i feel all weird when i have to meet w people ...      4\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/test.csv'\n",
        "\n",
        "test_data = pd.read_csv(file_path, sep=',')\n",
        "\n",
        "print(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBYPvqRO-21E",
        "outputId": "71b54c30-061b-44d0-98ff-b16f1b78d1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 4 3 2 5]\n"
          ]
        }
      ],
      "source": [
        "print(test_data['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM5N6edv06Wm",
        "outputId": "64ffb487-5345-4482-ef88-a72a677002fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0                                i didnt feel humiliated      0\n",
            "1      i can go from feeling so hopeless to so damned...      0\n",
            "2       im grabbing a minute to post i feel greedy wrong      3\n",
            "3      i am ever feeling nostalgic about the fireplac...      2\n",
            "4                                   i am feeling grouchy      3\n",
            "...                                                  ...    ...\n",
            "15995  i just had a very brief time in the beanbag an...      0\n",
            "15996  i am now turning and i feel pathetic that i am...      0\n",
            "15997                     i feel strong and good overall      1\n",
            "15998  i feel like this was such a rude comment and i...      3\n",
            "15999  i know a lot but i feel so stupid because i ca...      0\n",
            "\n",
            "[16000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTg61q3m7SFm",
        "outputId": "96874dd0-88d5-4836-c47c-67b71c37784d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label\n",
            "0     im feeling rather rotten so im not very ambiti...      0\n",
            "1             im updating my blog because i feel shitty      0\n",
            "2     i never make her separate from me because i do...      0\n",
            "3     i left with my bouquet of red and yellow tulip...      1\n",
            "4       i was feeling a little vain when i did this one      0\n",
            "...                                                 ...    ...\n",
            "1995  i just keep feeling like someone is being unki...      3\n",
            "1996  im feeling a little cranky negative after this...      3\n",
            "1997  i feel that i am useful to my people and that ...      1\n",
            "1998  im feeling more comfortable with derby i feel ...      1\n",
            "1999  i feel all weird when i have to meet w people ...      4\n",
            "\n",
            "[2000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "NOUjmrd_uMHw"
      },
      "outputs": [],
      "source": [
        "emotions = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emwHSLLpp3bi",
        "outputId": "32e23a61-e970-410a-cdc1-b87e99e1fda3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "EzAGDjX1qEH4",
        "outputId": "267bad39-414b-4c68-df3c-3e307f86735c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "#mising values\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU8hkLXqqId4",
        "outputId": "2aa83c14-4aaa-45a1-91db-b7e396eaf1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c88j7ic07Zte",
        "outputId": "bf1865f0-db10-47ec-dcf9-2942dc6d5ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 3 2 5 4 1]\n"
          ]
        }
      ],
      "source": [
        "print(data['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "z-DFLOF_qOy1",
        "outputId": "f9f8b65e-ff05-4d38-c527-d42062f993e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "type(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54hs3QBBqzFE",
        "outputId": "b2f8a89d-1178-47e4-9b99-45e614b21314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label  \\\n",
            "0                                i didnt feel humiliated      0   \n",
            "1      i can go from feeling so hopeless to so damned...      0   \n",
            "2       im grabbing a minute to post i feel greedy wrong      3   \n",
            "3      i am ever feeling nostalgic about the fireplac...      2   \n",
            "4                                   i am feeling grouchy      3   \n",
            "...                                                  ...    ...   \n",
            "15995  i just had a very brief time in the beanbag an...      0   \n",
            "15996  i am now turning and i feel pathetic that i am...      0   \n",
            "15997                     i feel strong and good overall      1   \n",
            "15998  i feel like this was such a rude comment and i...      3   \n",
            "15999  i know a lot but i feel so stupid because i ca...      0   \n",
            "\n",
            "                                         cleaned_content  \n",
            "0                                  didnt feel humiliated  \n",
            "1      go feeling hopeless damned hopeful around some...  \n",
            "2              im grabbing minute post feel greedy wrong  \n",
            "3      ever feeling nostalgic fireplace know still pr...  \n",
            "4                                        feeling grouchy  \n",
            "...                                                  ...  \n",
            "15995      brief time beanbag said anna feel like beaten  \n",
            "15996  turning feel pathetic still waiting tables sub...  \n",
            "15997                           feel strong good overall  \n",
            "15998                     feel like rude comment im glad  \n",
            "15999                       know lot feel stupid portray  \n",
            "\n",
            "[16000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define stopwords and helper function for tokenization\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"Error: Stopwords not found. Please ensure nltk.stopwords is downloaded.\")\n",
        "    stop_words = set()\n",
        "\n",
        "\n",
        "# Apply cleaning using a lambda function\n",
        "data['cleaned_content'] = data['text'].apply(\n",
        "    lambda x: ' '.join([\n",
        "        word for word in word_tokenize(\n",
        "            re.sub(r'\\s+', ' ',  # Remove additional spaces\n",
        "            re.sub(r'<.*?>', '',  # Remove HTML tags\n",
        "            re.sub(r'http\\S+|www\\S+|https\\S+', '',  # Remove URLs\n",
        "            re.sub(r'[^a-zA-Z\\s]', '',  # Remove special characters\n",
        "            x.lower())))).strip())  # Convert to lowercase\n",
        "        if word not in stop_words  # Remove stopwords\n",
        "    ])\n",
        ")\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVrs4SHErJUt",
        "outputId": "9794977f-9ddf-44c3-c53b-e4a3db198f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label  \\\n",
            "0                                i didnt feel humiliated      0   \n",
            "1      i can go from feeling so hopeless to so damned...      0   \n",
            "2       im grabbing a minute to post i feel greedy wrong      3   \n",
            "3      i am ever feeling nostalgic about the fireplac...      2   \n",
            "4                                   i am feeling grouchy      3   \n",
            "...                                                  ...    ...   \n",
            "15995  i just had a very brief time in the beanbag an...      0   \n",
            "15996  i am now turning and i feel pathetic that i am...      0   \n",
            "15997                     i feel strong and good overall      1   \n",
            "15998  i feel like this was such a rude comment and i...      3   \n",
            "15999  i know a lot but i feel so stupid because i ca...      0   \n",
            "\n",
            "                                         cleaned_content  \n",
            "0                                   didnt feel humiliate  \n",
            "1      go feeling hopeless damn hopeful around someon...  \n",
            "2                  im grab minute post feel greedy wrong  \n",
            "3      ever feel nostalgic fireplace know still property  \n",
            "4                                           feel grouchy  \n",
            "...                                                  ...  \n",
            "15995       brief time beanbag say anna feel like beaten  \n",
            "15996  turn feel pathetic still wait table sub teach ...  \n",
            "15997                           feel strong good overall  \n",
            "15998                     feel like rude comment im glad  \n",
            "15999                       know lot feel stupid portray  \n",
            "\n",
            "[16000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('punkt')\n",
        "# Download the averaged_perceptron_tagger_eng resource\n",
        "download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for POS tagging and lemmatization\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join(\n",
        "        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
        "        for word, tag in pos_tag(word_tokenize(text))\n",
        "    )\n",
        "\n",
        "# Helper: Convert POS tags to WordNet format\n",
        "def get_wordnet_pos(tag):\n",
        "    return {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }.get(tag[0], wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization\n",
        "data['cleaned_content'] = data['cleaned_content'].apply(lemmatize_text)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD3Rb8QxAr-q",
        "outputId": "8ce87850-c8d3-4e3b-e379-a0e269c8c586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label  \\\n",
            "0     im feeling rather rotten so im not very ambiti...      0   \n",
            "1             im updating my blog because i feel shitty      0   \n",
            "2     i never make her separate from me because i do...      0   \n",
            "3     i left with my bouquet of red and yellow tulip...      1   \n",
            "4       i was feeling a little vain when i did this one      0   \n",
            "...                                                 ...    ...   \n",
            "1995  i just keep feeling like someone is being unki...      3   \n",
            "1996  im feeling a little cranky negative after this...      3   \n",
            "1997  i feel that i am useful to my people and that ...      1   \n",
            "1998  im feeling more comfortable with derby i feel ...      1   \n",
            "1999  i feel all weird when i have to meet w people ...      4   \n",
            "\n",
            "                                        cleaned_content  \n",
            "0           im feeling rather rotten im ambitious right  \n",
            "1                          im updating blog feel shitty  \n",
            "2       never make separate ever want feel like ashamed  \n",
            "3     left bouquet red yellow tulips arm feeling sli...  \n",
            "4                               feeling little vain one  \n",
            "...                                                 ...  \n",
            "1995  keep feeling like someone unkind wrong think g...  \n",
            "1996  im feeling little cranky negative doctors appo...  \n",
            "1997  feel useful people gives great feeling achieve...  \n",
            "1998  im feeling comfortable derby feel though start...  \n",
            "1999  feel weird meet w people text like dont talk f...  \n",
            "\n",
            "[2000 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define stopwords and helper function for tokenization\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"Error: Stopwords not found. Please ensure nltk.stopwords is downloaded.\")\n",
        "    stop_words = set()\n",
        "\n",
        "\n",
        "\n",
        "# Apply cleaning using a lambda function\n",
        "test_data['cleaned_content'] = test_data['text'].apply(\n",
        "    lambda x: ' '.join([\n",
        "        word for word in word_tokenize(\n",
        "            re.sub(r'\\s+', ' ',  # Remove additional spaces\n",
        "            re.sub(r'<.*?>', '',  # Remove HTML tags\n",
        "            re.sub(r'http\\S+|www\\S+|https\\S+', '',  # Remove URLs\n",
        "            re.sub(r'[^a-zA-Z\\s]', '',  # Remove special characters\n",
        "            x.lower())))).strip())  # Convert to lowercase\n",
        "        if word not in stop_words  # Remove stopwords\n",
        "    ])\n",
        ")\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONn3zAiRA_lH",
        "outputId": "e03dadb6-5c7b-4f5a-f2c5-5bffa6ae9240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  label  \\\n",
            "0     im feeling rather rotten so im not very ambiti...      0   \n",
            "1             im updating my blog because i feel shitty      0   \n",
            "2     i never make her separate from me because i do...      0   \n",
            "3     i left with my bouquet of red and yellow tulip...      1   \n",
            "4       i was feeling a little vain when i did this one      0   \n",
            "...                                                 ...    ...   \n",
            "1995  i just keep feeling like someone is being unki...      3   \n",
            "1996  im feeling a little cranky negative after this...      3   \n",
            "1997  i feel that i am useful to my people and that ...      1   \n",
            "1998  im feeling more comfortable with derby i feel ...      1   \n",
            "1999  i feel all weird when i have to meet w people ...      4   \n",
            "\n",
            "                                        cleaned_content  \n",
            "0              im feel rather rotten im ambitious right  \n",
            "1                            im update blog feel shitty  \n",
            "2       never make separate ever want feel like ashamed  \n",
            "3     leave bouquet red yellow tulip arm feeling sli...  \n",
            "4                                  feel little vain one  \n",
            "...                                                 ...  \n",
            "1995  keep feeling like someone unkind wrong think g...  \n",
            "1996  im feel little cranky negative doctor appointment  \n",
            "1997  feel useful people give great feeling achievement  \n",
            "1998  im feel comfortable derby feel though start st...  \n",
            "1999  feel weird meet w people text like dont talk f...  \n",
            "\n",
            "[2000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('wordnet')\n",
        "download('punkt')\n",
        "# Download the averaged_perceptron_tagger_eng resource\n",
        "download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for POS tagging and lemmatization\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join(\n",
        "        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
        "        for word, tag in pos_tag(word_tokenize(text))\n",
        "    )\n",
        "\n",
        "# Helper: Convert POS tags to WordNet format\n",
        "def get_wordnet_pos(tag):\n",
        "    return {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }.get(tag[0], wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization\n",
        "test_data['cleaned_content'] = test_data['cleaned_content'].apply(lemmatize_text)\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K28rsSTRwkmY",
        "outputId": "756e5ed9-82b7-4305-e0b9-c6862a354cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.1450 - loss: 2.1820 - val_accuracy: 0.2244 - val_loss: 1.7426\n",
            "Epoch 2/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.2473 - loss: 1.6562 - val_accuracy: 0.5497 - val_loss: 1.1990\n",
            "Epoch 3/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.7622 - loss: 0.6275 - val_accuracy: 0.8662 - val_loss: 0.4717\n",
            "Epoch 4/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9253 - loss: 0.2524 - val_accuracy: 0.8813 - val_loss: 0.4309\n",
            "Epoch 5/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 23ms/step - accuracy: 0.9498 - loss: 0.1471 - val_accuracy: 0.8791 - val_loss: 0.4352\n",
            "Epoch 6/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9644 - loss: 0.1038 - val_accuracy: 0.8803 - val_loss: 0.4173\n",
            "Epoch 7/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9710 - loss: 0.0800 - val_accuracy: 0.8838 - val_loss: 0.4109\n",
            "Epoch 8/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.9788 - loss: 0.0641 - val_accuracy: 0.8869 - val_loss: 0.4244\n",
            "Epoch 9/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.9839 - loss: 0.0476 - val_accuracy: 0.8806 - val_loss: 0.4930\n",
            "Epoch 10/20\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9844 - loss: 0.0463 - val_accuracy: 0.8794 - val_loss: 0.4761\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8846 - loss: 0.4241\n",
            "Test Loss: 0.4435\n",
            "Test Accuracy: 0.8750\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow nltk scikit-learn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "X_train=data['cleaned_content']\n",
        "y_train=data['label']\n",
        "X_test=test_data['cleaned_content']\n",
        "y_test=test_data['label']\n",
        "\n",
        "\n",
        "# 3. Tokenize text\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 4. Convert text to sequences and pad\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_length = 100  # Maximum sequence length (adjust as needed)\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# 5. Build the LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True, kernel_initializer=he_normal())),\n",
        "    Dropout(0.6),\n",
        "    Bidirectional(LSTM(32,  kernel_initializer=he_normal())),\n",
        "    Dense(28, activation='softmax')  # Output layer with 27 classes for your emotion labels\n",
        "])\n",
        "\n",
        "# 6. Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "# model.fit(X_train_padded, y_train, epochs=1, batch_size=64, validation_split=0.98)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                     classes=np.unique(y_train),\n",
        "                                     y=y_train)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "# Pass class_weights_dict to the fit method\n",
        "model.fit(X_train_padded, y_train, epochs=20, batch_size=64,\n",
        "          validation_split=0.2, class_weight=class_weights_dict,\n",
        "          callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 8. Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# Save the tokenizer to a file\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Tokenizer saved successfully!\")"
      ],
      "metadata": {
        "id": "liSYtgwUt1RW",
        "outputId": "2905b563-c89b-4092-eceb-3726fc025581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OIBtTaGJsgS",
        "outputId": "47b9419c-eec8-49d2-c42f-8b500fc17670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text i am angry\n",
            "text_padded [[170   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Predicted class index: 3\n",
            "Predicted Emotion: anger\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the lemmatizer and stopwords list\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and preprocess the input text\n",
        "def preprocess_input(text):\n",
        "    print(\"text\",text)\n",
        "    # Remove additional spaces, HTML tags, URLs, and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove additional spaces\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the cleaned words back into a sentence\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "\n",
        "    # Tokenize and pad the cleaned text\n",
        "    text_seq = tokenizer.texts_to_sequences([cleaned_text])  # Convert text to sequence\n",
        "    text_padded = pad_sequences(text_seq, maxlen=max_length, padding='post', truncating='post')  # Pad sequence\n",
        "    print(\"text_padded\",text_padded)\n",
        "    return text_padded\n",
        "\n",
        "# Sample sentence to predict\n",
        "sentence = \"i am angry\"\n",
        "\n",
        "# Preprocess the input sentence\n",
        "preprocessed_input = preprocess_input(sentence)\n",
        "# print(\"preprocess_input\",preprocessed_input)\n",
        "\n",
        "# Predict using the trained model\n",
        "prediction = model.predict(preprocessed_input)\n",
        "# print(prediction)\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "print(f\"Predicted class index: {predicted_class}\")\n",
        "\n",
        "# Assuming 'emotions' is a list of emotion labels\n",
        "answer = emotions[predicted_class]\n",
        "print(f\"Predicted Emotion: {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkurWG5VQOL8",
        "outputId": "038c8de3-1723-4ffc-ef00-e14b9584ca4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_padded [[166   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "[[4.42188740e-01 4.05263543e-01 3.70147228e-02 6.55944943e-02\n",
            "  3.59691679e-02 9.56021342e-03 2.76098988e-04 3.11127515e-05\n",
            "  3.88282177e-04 3.51410185e-04 9.12586693e-05 8.75693149e-05\n",
            "  2.31315877e-04 1.66261365e-04 1.15450479e-04 1.46354811e-04\n",
            "  1.69841267e-04 9.30603637e-05 4.35617520e-04 1.86994017e-04\n",
            "  1.11548106e-04 4.30546061e-04 1.57378265e-04 1.62106866e-04\n",
            "  4.63036013e-05 2.02191266e-04 2.13620093e-04 3.14801757e-04]]\n",
            "Predicted class index: 0\n",
            "Predicted Emotion: sadness\n"
          ]
        }
      ],
      "source": [
        "# Sample sentence to predict\n",
        "sentence = \"very sad\"\n",
        "\n",
        "# Preprocess the input sentence\n",
        "preprocessed_input = preprocess_input(sentence)\n",
        "\n",
        "# Predict using the trained model\n",
        "prediction = model.predict(preprocessed_input)\n",
        "print(prediction)\n",
        "\n",
        "# Get the predicted class index\n",
        "# Remove the slicing operation on prediction\n",
        "predicted_class = np.argmax(prediction[0]) # find argmax for the first (and only) prediction\n",
        "print(f\"Predicted class index: {predicted_class}\")\n",
        "\n",
        "# Assuming 'emotions' is a list of emotion labels\n",
        "# Adjust index to access the correct emotion from the list\n",
        "answer = emotions[predicted_class]\n",
        "print(f\"Predicted Emotion: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"emotion.h5\");"
      ],
      "metadata": {
        "id": "JewAnyMSVH8c",
        "outputId": "93fa9513-827a-43b0-8adc-dcc80456d746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "_gWd7ewIYKV7",
        "outputId": "816c637f-b41c-46e6-da81-7e2718a0e02b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "6WWV25RwYOZt",
        "outputId": "3cb5fa2c-5ff3-48ed-9be4-20d1f236ad8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.124.217.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-XfHg19oYn3I",
        "outputId": "b15d9127-335c-4067-fc8a-84c02f1f7e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.24.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "pDBRzr_jYYM8"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "B_dEVD7kYbEA",
        "outputId": "25e40412-7dde-4374-9392-fc149491506a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://chilly-chefs-wink.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}